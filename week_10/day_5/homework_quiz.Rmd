---
title: "Homework Quiz"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br><br>

1. I want to predict how well 6 year-olds are going to do in their final school exams. Using the following variables am I likely under-fitting, fitting well or over-fitting? Postcode, gender, reading level, score in maths test, date of birth, family income.
You are likely to over-fitting.

2. If I have two models, one with an AIC score of 34,902 and the other with an AIC score of 33,559 which model should I use?

The one with the lower score (33,559).


3. I have two models, the first with: r-squared: 0.44, adjusted r-squared: 0.43. The second with: r-squared: 0.47, adjusted r-squared: 0.41. Which one should I use?

It depends what is more important to you - the second model explains 47% of the variance in the dependent variable, however adjusted r-squared at 41% suggests that the model is over-fitted.
The first model though seems to be simpler and more robust - it explains 44% variance in the dependent variable, with a drop to 43% when considering adjusted r_squared.

I would choose model one as it is more robust. 

4. I have a model with the following errors: 
RMSE error on test set: 10.3, 
RMSE error on training data: 10.4. 
Do you think this model is over-fitting?
No, I do not think the model is over-fitting as both errors are very close to each other.

5. How does k-fold validation work?

The process involves at first splitting the data set into k-number of subsets(folds). Each subset contains approximately the same number of samples chosen at random. Then the iterative process starts, which means that in each iteration one of the subsets is used as the test set and the rest as a training set. So model is being trained on the training set and tested on the test set. After repeating the process k-times there will be an average result produced.

6. What is a validation set? When do you need one?

The validation set is another extra set in addition to test and training set - it has never been used to train or compare different models with each other. The validation set should provide a final estimate of the expected performance of the model and should be used only once, when the final model has already been selected.
The validation model is needed in order to check whether the model makes accurate predictions beyond data it was trained and tested on. 


7. Describe how backwards selection works.
It starts with the model containing all the possible predictors and at each step all predictors are being checked and the one with the lowest r_squared gets removed. At the end we get a list of models with varying predictor number. Backwards selection would only work if there is at least as many data point in the sample as there are predictors.

8. Describe how best subset selection works.

It is also called the exhaustive search and is being used to provide the optimal model. At first you set up the maximum number of predictors you would like the model to have and then all possible combinations of predictors are being generated by a computer and checked for the best set of features for the model. The results involves the model with the best performance based on the chosen evaluation - usually r-squared.





